{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Distribution, self).__init__()\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        raise NotImplementedError(\"Log Prob Not Allowed\")\n",
    "        \n",
    "    def sample(self):\n",
    "        raise NotImplementedError(\"Sampling Not Allowed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(Distribution):\n",
    "    \n",
    "    def __init__(self, means=None, stds=None, dims=1):\n",
    "        super(Normal, self).__init__()\n",
    "        if means is None:\n",
    "            self.mu = Parameter(torch.Tensor(1, dims).uniform_(-1, 1))\n",
    "        else:\n",
    "            self.mu = means\n",
    "        if stds is None:\n",
    "            self.std = Parameter(torch.Tensor(1, dims).uniform_(0, 1))\n",
    "        else:\n",
    "            self.std = stds\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        prob = torch.exp(-((x-self.mu)**2) / (2*(self.std**2)))\n",
    "        prob = (1.0 / torch.sqrt(2 * math.pi * (self.std**2))) * prob\n",
    "        return torch.log(prob+1e-10)\n",
    "    \n",
    "    def _sample(self):\n",
    "        return self.mu + self.std * torch.randn(self.std.size())\n",
    "    \n",
    "    def sample(self, n=1):\n",
    "        return torch.cat([self._sample() for _ in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = Normal(dims=1)\n",
    "plt.hist([n.sample().item() for _ in range(10000)], bins=100)\n",
    "# values = np.array([n.sample().detach().numpy()[0] for _ in range(1000)])\n",
    "# plt.plot(n.sample()\n",
    "# plt.scatter(values[:, 0], values[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardKL(samples, distribution):\n",
    "    return -distribution.log_prob(samples).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Normal(dims=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.normal(mean=torch.zeros(64).fill_(-2.5), \n",
    "                       std=torch.ones(64).fill_(6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.log_prob(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 790.093\n",
      "Epoch 100 Loss: 424.128\n",
      "Epoch 200 Loss: 327.702\n",
      "Epoch 300 Loss: 274.807\n",
      "Epoch 400 Loss: 249.329\n",
      "Epoch 500 Loss: 235.703\n",
      "Epoch 600 Loss: 227.191\n",
      "Epoch 700 Loss: 221.405\n",
      "Epoch 800 Loss: 217.258\n",
      "Epoch 900 Loss: 214.173\n"
     ]
    }
   ],
   "source": [
    "for i in range(1500):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = ForwardKL(samples, model)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i} Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.5851]], requires_grad=True) Parameter containing:\n",
      "tensor([[5.1270]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.mu, model.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal_No_Sample(Distribution):\n",
    "    \n",
    "    def __init__(self, means=None, stds=None, dims=1):\n",
    "        super(Normal_No_Sample, self).__init__()\n",
    "        if means is None:\n",
    "            self.mu = Parameter(torch.Tensor(1, dims).uniform_(-1, 1))\n",
    "        else:\n",
    "            self.mu = means\n",
    "        if stds is None:\n",
    "            self.std = Parameter(torch.Tensor(1, dims).uniform_(0, 1))\n",
    "        else:\n",
    "            self.std = stds\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        prob = torch.exp(-((x-self.mu)**2) / (2*(self.std**2)))\n",
    "        prob = (1.0 / torch.sqrt(2 * math.pi * (self.std**2))) * prob\n",
    "        return torch.log(prob+1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[5.]], grad_fn=<CopySlices>) Parameter containing:\n",
      "tensor([[2.]], grad_fn=<CopySlices>)\n",
      "Parameter containing:\n",
      "tensor([[0.7807]], requires_grad=True) Parameter containing:\n",
      "tensor([[0.1378]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "truth = Normal_No_Sample()\n",
    "model = Normal()\n",
    "\n",
    "print(truth.mu, truth.std)\n",
    "print(model.mu, model.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReverseKL(q, p, batch_size=64):\n",
    "    samples = q.sample(batch_size)\n",
    "    return -(p.log_prob(samples) - q.log_prob(samples)).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "leaf variable has been moved into the graph interior",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-397-72102c127bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReverseKL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: leaf variable has been moved into the graph interior"
     ]
    }
   ],
   "source": [
    "for i in range(1500):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = ReverseKL(model, truth)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i} Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[5.]], grad_fn=<CopySlices>) Parameter containing:\n",
      "tensor([[2.]], grad_fn=<CopySlices>)\n",
      "Parameter containing:\n",
      "tensor([[0.7807]], requires_grad=True) Parameter containing:\n",
      "tensor([[0.1378]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(truth.mu, truth.std)\n",
    "print(model.mu, model.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
